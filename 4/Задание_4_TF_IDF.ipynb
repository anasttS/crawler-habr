{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPa0jjKIrHz"
      },
      "source": [
        "### Задание 4 - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvmBYlI9LbSE"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToV-e6qONMdd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "import pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(content):\n",
        "    tokens_ = nltk.word_tokenize(content)\n",
        "    tokens_ = [i.lower() for i in tokens_]\n",
        "    tokens_ = [i for i in tokens_ if ( i not in string.punctuation )]\n",
        "    tokens_ = filter(None, [re.sub(r\"[^a-zа-я-]+\", r\"\", i) for i in tokens_])\n",
        "\n",
        "    stop_words = stopwords.words('russian')\n",
        "    stop_words.extend(['что', 'это', 'так', 'вот', 'как', 'в', 'к',\n",
        "                         'на', 'о', 'при', 'из-за', 'за', 'ао', 'но', 'х',\n",
        "                         'хотя', 'среди', 'помимо', 'с'])\n",
        "    tokens_ = [i for i in tokens_ if ( i not in stop_words )]\n",
        "\n",
        "    stop_words2 = stopwords.words('english')\n",
        "    stop_words2.extend(['the', 'e', 'a', 'd', 'b', 'x', 'c'])\n",
        "    tokens_ = [i for i in tokens_ if ( i not in stop_words2 )]\n",
        "\n",
        "    tokens_ = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens_]\n",
        "\n",
        "    return tokens_"
      ],
      "metadata": {
        "id": "J4bTLQalUig4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qBMVDd8XzpPa"
      },
      "outputs": [],
      "source": [
        "def tokenize_list_content(content):\n",
        "    result = []\n",
        "    tokens = set() \n",
        "    for c in content:\n",
        "      tokens_ = tokenize(c)\n",
        "      for t_ in tokens_:\n",
        "        tokens.add(t_)\n",
        "      result += [tokens]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "RBIr_t2wNb-0"
      },
      "outputs": [],
      "source": [
        "# словарь (номер документа -> количество слов (контент (заголовок + содержимое статьи)))\n",
        "content_count_of_words = {}\n",
        "content_words = {}\n",
        "content_ = []\n",
        "content = {}\n",
        "\n",
        "i = 1\n",
        "for file_ in os.listdir('links'):\n",
        "  if (file_.__contains__('.html')):\n",
        "    path_ = os.path.join('links', file_)\n",
        "    file_text = open(path_).read()\n",
        "    \n",
        "    soup = BeautifulSoup(file_text)\n",
        "    words = set()\n",
        "    page_content = []\n",
        "\n",
        "    for text in soup.find('h1', {'class': 'tm-article-snippet__title tm-article-snippet__title_h1'}):\n",
        "      title = text.get_text()\n",
        "      title_tokens = tokenize(title)\n",
        "      for token in title_tokens:\n",
        "        words.add(token)\n",
        "      page_content.append(title)\n",
        "\n",
        "    data = soup.find('div', {'xmlns': 'http://www.w3.org/1999/xhtml'})\n",
        "    for p in data.find_all('p'):\n",
        "       text = p.get_text()\n",
        "       text_tokens = tokenize(text)\n",
        "       for token in text_tokens:\n",
        "          words.add(token)\n",
        "       page_content.append(text)\n",
        "    \n",
        "    sum = 0\n",
        "    for str_ in page_content:\n",
        "      sum += len(str_.split())\n",
        "\n",
        "    content_words[i] = words\n",
        "    content[i] = page_content\n",
        "    content_.append(page_content)\n",
        "    content_count_of_words[i] = sum\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yonRULJ5taTQ"
      },
      "outputs": [],
      "source": [
        "lemmas_ = open('lemmas.txt').read()\n",
        "lemmas_ = lemmas_.split('\\n')\n",
        "lemmas = {}\n",
        "for l in lemmas_:\n",
        "  l = l.split(':')\n",
        "  lemmas[l[0]] = l[1].split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B5UF5EiGTsjb"
      },
      "outputs": [],
      "source": [
        "# создадим индекс лемм (индекс токенов был сделан в задании 3)\n",
        "import json \n",
        "with open('inverted_index.txt', encoding=\"utf-8\") as file:\n",
        "    rows = [row.strip() for row in file]\n",
        "\n",
        "index_tokens = {}\n",
        "for row in rows:\n",
        "    row = row.replace('\\'', '\\\"')\n",
        "    dict_ = json.loads(row)\n",
        "    index_tokens[dict_[\"word\"]] = [dict_[\"count\"], dict_[\"inverted_array\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P1x-bCd5287e"
      },
      "outputs": [],
      "source": [
        "import pymorphy2\n",
        "\n",
        "index_lemmas={}\n",
        "lemmas_list = list(dict.fromkeys(lemmas))\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "for i in range(1, 101):\n",
        "    data = []\n",
        "    for c in tokenize_list_content(content[i]):\n",
        "        data += c\n",
        "    data = list(dict.fromkeys(data))\n",
        "    for token in data:\n",
        "        lemma_from_token = morph.parse(token)[0].normal_form\n",
        "        if lemma_from_token in index_lemmas.keys():\n",
        "            index_lemmas[lemma_from_token].append(i)\n",
        "        else:\n",
        "            index_lemmas[lemma_from_token] = [i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Вспомогательные функции для подсчета tf-idf\n"
      ],
      "metadata": {
        "id": "birj3GsLX1Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_from_doc(id):\n",
        "  tokens = []\n",
        "  for token in index_tokens:\n",
        "    if (id in index_tokens[token][1]):\n",
        "      tokens.append(token)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "xsJvSMxFxU2p"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmas_from_doc(id):\n",
        "  lemmas = []\n",
        "  for lemma in index_lemmas:\n",
        "    if (id in index_lemmas[lemma]):\n",
        "      lemmas.append(lemma)\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "hPP__hZCrEXZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmas(tokens): \n",
        "  lemmas = {}\n",
        "  for token in tokens:\n",
        "    lemma = morph.parse(token)[0].normal_form\n",
        "    flag = lemmas.get(lemma)\n",
        "    if flag is None:\n",
        "      lemmas[lemma] = token\n",
        "    else:\n",
        "      if token not in flag:\n",
        "        lemmas[lemma] = flag + ' ' + token\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "HQHSlyquQisM"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# tf(t,d) = count of t in d / number of words in d\n",
        "def find_tf_tokens(content, id):\n",
        "    tokens_tfs = {}\n",
        "    tokens_counter = collections.Counter(content)\n",
        "    for i in tokens_counter:\n",
        "        tokens_tfs[i] = tokens_counter[i] / content_count_of_words[id]\n",
        "    return tokens_tfs\n",
        "\n",
        "# idf(t) = log(N / (df + 1))\n",
        "def find_idf_tokens(content_words):\n",
        "  tokens_idf = {}\n",
        "  for token in content_words:\n",
        "    df_tokens = set()\n",
        "    for id in content:\n",
        "      if token in get_tokens_from_doc(id):\n",
        "        df_tokens.add(id)\n",
        "    tokens_idf[token] = math.log10(100 / len(df_tokens))\n",
        "  return tokens_idf"
      ],
      "metadata": {
        "id": "OalY61XXoWvz"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf(t,d) = count of l in d / number of words in d\n",
        "def find_tf_lemmas(lemmas, content, id):\n",
        "  lemmas_tfs = {}\n",
        "  content = ' '.join(content)\n",
        "  for l in lemmas:\n",
        "    count_of_l = 0\n",
        "    for value in lemmas[l]:\n",
        "      count_of_l += content.count(l)\n",
        "    lemmas_tfs[l] = count_of_l / content_count_of_words[id]\n",
        "  return lemmas_tfs\n",
        "\n",
        "# idf(t) = log(N / (df + 1))\n",
        "def find_idf_lemmas(lemmas):\n",
        "  lemmas_idf = {}\n",
        "  for l in lemmas:\n",
        "    df_lemmas = set()\n",
        "    for id in content:\n",
        "      if l in get_lemmas_from_doc(id):\n",
        "        df_lemmas.add(id)\n",
        "    lemmas_idf[l] = math.log10(100 / len(df_lemmas))\n",
        "  return lemmas_idf"
      ],
      "metadata": {
        "id": "bO5uOCbvqEhG"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "2_vWqaXp-5kU"
      },
      "outputs": [],
      "source": [
        "for id in content_words:\n",
        "  tokens_tf = find_tf_tokens(content_words[id], id)\n",
        "  tokens_idf = find_idf_tokens(content_words[id])\n",
        "  \n",
        "  file_t = open('tf-idf_tokens/' + str(id) + '.txt', 'w+')\n",
        "  for token in content_words[id]:\n",
        "    tfidf = tokens_tf[token] * tokens_idf[token]\n",
        "    file_t.write(token + ' ' + str(tokens_idf[token]) + ' ' + str(tfidf) + '\\n')\n",
        "  file_t.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id in content_words:\n",
        "  lemmas = get_lemmas(content_words[id])\n",
        "\n",
        "  lemmas_tf = find_tf_lemmas(lemmas, content[id], id)\n",
        "  lemmas_idf = find_idf_lemmas(lemmas)\n",
        "\n",
        "  file_l = open('tf-idf_lemmas/' + str(id) + '.txt', 'w+')\n",
        "  for lemma in lemmas:\n",
        "    tfidf = lemmas_tf[lemma] * lemmas_idf[lemma]\n",
        "    file_l.write(lemma + ' ' + str(lemmas_idf[lemma]) + ' ' + str(tfidf) + '\\n')\n",
        "  file_l.close()"
      ],
      "metadata": {
        "id": "2hglFn-wGa03"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r tf-idf_tokens.zip tf-idf_tokens"
      ],
      "metadata": {
        "id": "7kxjW-uzO3Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r tf-idf_lemmas.zip tf-idf_lemmas"
      ],
      "metadata": {
        "id": "wiIz5BXPUWwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "sj9VoDv_tC-s"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"tf-idf_tokens.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CbbuMc9YPHX2",
        "outputId": "a692e7cb-9829-4ade-c388-83423454ea03"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_50492067-0403-4f1f-8b8d-5c1eacb9badb\", \"tf-idf_tokens.zip\", 146072)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"tf-idf_lemmas.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7EdyFdvwUcKW",
        "outputId": "fe8fba56-5dde-43f8-835a-da4bc43a17ec"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2099b8fa-abf6-498d-b12c-98ec9e157184\", \"tf-idf_lemmas.zip\", 153962)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Задание 4 - TF-IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}